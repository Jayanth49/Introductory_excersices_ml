{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import custom\n",
    "\n",
    "import random\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = custom.load_data_time_machine(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(vocab_size, num_hiddens, device):\n",
    "    num_inputs = num_outputs = vocab_size\n",
    "\n",
    "    def normal(shape):\n",
    "        return torch.randn(size=shape, device=device)*0.01\n",
    "\n",
    "    def three():\n",
    "        return (normal((num_inputs, num_hiddens)),\n",
    "                normal((num_hiddens, num_hiddens)),\n",
    "                torch.zeros(num_hiddens, device=device))\n",
    "\n",
    "    W_xz, W_hz, b_z = three()  # Update gate parameters\n",
    "    W_xr, W_hr, b_r = three()  # Reset gate parameters\n",
    "    W_xh, W_hh, b_h = three()  # Candidate hidden state parameters\n",
    "    # Output layer parameters\n",
    "    W_hq = normal((num_hiddens, num_outputs))\n",
    "    b_q = torch.zeros(num_outputs, device=device)\n",
    "    # Attach gradients\n",
    "    params = [W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q]\n",
    "    for param in params:\n",
    "        param.requires_grad_(True)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_gru_state(batch_size, num_hiddens, device):\n",
    "    return (torch.zeros((batch_size, num_hiddens), device=device), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru(inputs, state, params):\n",
    "    W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    H, = state\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        Z = torch.sigmoid((X @ W_xz) + (H @ W_hz) + b_z)\n",
    "        R = torch.sigmoid((X @ W_xr) + (H @ W_hr) + b_r)\n",
    "        H_tilda = torch.tanh((X @ W_xh) + ((R * H) @ W_hh) + b_h)\n",
    "        H = Z * H + (1 - Z) * H_tilda\n",
    "        Y = H @ W_hq + b_q\n",
    "        outputs.append(Y)\n",
    "    return torch.cat(outputs, dim=0), (H,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time traveller                                                  \n",
      "time traveller ate ate ate ate ate ate ate ate ate ate ate ate a\n",
      "time travellere the athe the athe the athe the athe the athe the\n",
      "time traveller the the the the the the the the the the the the t\n",
      "time travellere the the the the the the the the the the the the \n",
      "time travellere the the the the the the the the the the the the \n",
      "time traveller an the the the the the the the the the the the th\n",
      "time travellerererererererererererererererererererererererererer\n",
      "time travellerererererererererererererererererererererererererer\n",
      "time traveller and the the the the the the the the the the the t\n",
      "time travellere the the the the the the the the the the the the \n",
      "time traveller the the the the the the the the the the the the t\n",
      "time travellere the the the the the the the the the the the the \n",
      "time traveller and the the the the the the the the the the the t\n",
      "time travellere the the the the the the the the the the the the \n",
      "time travellere and the the the the the the the the the the the \n",
      "time traveller and the the the the the the the the the the the t\n",
      "time traveller and the the the the the the the the the the the t\n",
      "time traveller the the the the the the the the the the the the t\n",
      "time travellered and and the time travellered and and the time t\n",
      "time traveller the the the the the the the the the the the the t\n",
      "time traveller the the the the the the the the the the the the t\n",
      "time traveller the time traveller the time traveller the time tr\n",
      "time traveller that is and so ment only in that is and so ment o\n",
      "time traveller that is and so medine in a monis be and that is a\n",
      "time traveller the time traveller the time traveller the time tr\n",
      "time traveller the time traveller the time traveller the time tr\n",
      "time travellerit timets as in time as all hard and seatly there \n",
      "time travellers and the time traveller to so a sainot and the ti\n",
      "time traveller the time traveller the time traveller the time tr\n",
      "time traveller things a contly i sumant bly i shall ther think s\n",
      "time traveller to me ans one of the staroul a that is a four a s\n",
      "time traveller some and pe have extension another at wey souldow\n",
      "time traveller pecpees tound thes to be and so i ned man atory t\n",
      "time travellerit s amainst buc an furmey wesc that is a move a l\n",
      "time traveller followire wright and we have no means of staying \n",
      "time traveller for so it will be convenient to speak of himatsee\n",
      "time traveller follow you said filbycan a cube that does not las\n",
      "time traveller scale buck very cim the paysomepsionspareed fir s\n",
      "time traveller four dimensions are peassient fol inspace but you\n",
      "time traveller followime some fore in thopshoushourd for said th\n",
      "time traveller proceeded anyreal body must have extension in fou\n",
      "time traveller frometh breedeed betcefion and hard dinens this i\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time traveller proceeded anyreal body must have extension in fou\n",
      "time traveller proceeded anyreal body must have extension in fou\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "perplexity 1.1, 8960.0 tokens/sec on cuda:0\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "travelleryou can show black is white by argument said filby\n"
     ]
    }
   ],
   "source": [
    "vocab_size, num_hiddens = len(vocab), 256\n",
    "num_epochs, lr = 500, 1\n",
    "model = custom.RNNModelScratch(len(vocab), num_hiddens, device, get_params,\n",
    "                            init_gru_state, gru)\n",
    "custom.train_ch8(model, train_iter, vocab, lr, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    \"\"\"The RNN model.\"\"\"\n",
    "    def __init__(self, rnn_layer, vocab_size, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        self.rnn = rnn_layer\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_hiddens = self.rnn.hidden_size\n",
    "        # If the RNN is bidirectional (to be introduced later),\n",
    "        # `num_directions` should be 2, else it should be 1.\n",
    "        if not self.rnn.bidirectional:\n",
    "            self.num_directions = 1\n",
    "            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)\n",
    "        else:\n",
    "            self.num_directions = 2\n",
    "            self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)\n",
    "\n",
    "    def forward(self, inputs, state):\n",
    "        X = F.one_hot(inputs.T.long(), self.vocab_size)\n",
    "        X = X.to(torch.float32)\n",
    "        Y, state = self.rnn(X, state)\n",
    "        # The fully connected layer will first change the shape of `Y` to\n",
    "        # (`num_steps` * `batch_size`, `num_hiddens`). Its output shape is\n",
    "        # (`num_steps` * `batch_size`, `vocab_size`).\n",
    "        output = self.linear(Y.reshape((-1, Y.shape[-1])))\n",
    "        return output, state\n",
    "\n",
    "    def begin_state(self, device, batch_size=1):\n",
    "        if not isinstance(self.rnn, nn.LSTM):\n",
    "            # `nn.GRU` takes a tensor as hidden state\n",
    "            return  torch.zeros((self.num_directions * self.rnn.num_layers,\n",
    "                                 batch_size, self.num_hiddens),\n",
    "                                device=device)\n",
    "        else:\n",
    "            # `nn.LSTM` takes a tuple of hidden states\n",
    "            return (torch.zeros((\n",
    "                self.num_directions * self.rnn.num_layers,\n",
    "                batch_size, self.num_hiddens), device=device),\n",
    "                    torch.zeros((\n",
    "                        self.num_directions * self.rnn.num_layers,\n",
    "                        batch_size, self.num_hiddens), device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time travellere te te te te te te te te te te te te te te te te \n",
      "time traveller the the the the the the the the the the the the t\n",
      "time travellere the the the the the the the the the the the the \n",
      "time traveller an the the the the the the the the the the the th\n",
      "time travellere the the the the the the the the the the the the \n",
      "time travellere the the the the the the the the the the the the \n",
      "time traveller the the the the the the the the the the the the t\n",
      "time traveller and the the the the the the the the the the the t\n",
      "time traveller the the the the the the the the the the the the t\n",
      "time traveller the the the the the the the the the the the the t\n",
      "time travellere the the the the the the the the the the the the \n",
      "time traveller and the there the the there the the there the the\n",
      "time traveller the the the the the the the the the the the the t\n",
      "time traveller and the the the the the the the the the the the t\n",
      "time traveller the the the the the the the the the the the the t\n",
      "time traveller and the the the the the the the the the the the t\n",
      "time traveller the this the three dimensions of the three dimens\n",
      "time traveller the three thing the three thing the three thing t\n",
      "time traveller three dimensions of the three dimensions of the t\n",
      "time traveller three dimensions and the time travel the time tra\n",
      "time traveller three dimensions of the three dimensions of the t\n",
      "time traveller three dimensions and the three dimensions and the\n",
      "time traveller that the mert co us and the time traveller that e\n",
      "time traveller three dimensions of space and timether time trave\n",
      "time traveller to dencut bot in time as we move about in the oth\n",
      "time traveller weth whycegeound bo no know of the bright light o\n",
      "time traveller well the grome to gestor a cubularestiracisor a s\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time traveller hal so convereer we can ghing the time traveller \n",
      "time traveller but now you begin to seethe object of my investig\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time travellerit s against reason said filbywhat so challsied al\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time travellerit s against reason said filbywhat so chay said th\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time travelleryou can show black is white by argument said filby\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time travelleryou can show black is white by argument said filby\n",
      "time travelleryou can show black is white by argument said filby\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "perplexity 1.0, 8960.0 tokens/sec on cuda:0\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "travelleryou can show black is white by argument said filby\n"
     ]
    }
   ],
   "source": [
    "num_inputs = vocab_size\n",
    "gru_layer = nn.GRU(num_inputs, num_hiddens)\n",
    "model = RNNModel(gru_layer, len(vocab))\n",
    "model = model.to(device)\n",
    "custom.train_ch8(model, train_iter, vocab, lr, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
