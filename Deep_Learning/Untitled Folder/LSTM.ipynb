{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import custom\n",
    "\n",
    "import random\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = custom.load_data_time_machine(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lstm_params(vocab_size, num_hiddens, device):\n",
    "    num_inputs = num_outputs = vocab_size\n",
    "\n",
    "    def normal(shape):\n",
    "        return torch.randn(size=shape, device=device)*0.01\n",
    "\n",
    "    def three():\n",
    "        return (normal((num_inputs, num_hiddens)),\n",
    "                normal((num_hiddens, num_hiddens)),\n",
    "                torch.zeros(num_hiddens, device=device))\n",
    "\n",
    "    W_xi, W_hi, b_i = three()  # Input gate parameters\n",
    "    W_xf, W_hf, b_f = three()  # Forget gate parameters\n",
    "    W_xo, W_ho, b_o = three()  # Output gate parameters\n",
    "    W_xc, W_hc, b_c = three()  # Candidate memory cell parameters\n",
    "    # Output layer parameters\n",
    "    W_hq = normal((num_hiddens, num_outputs))\n",
    "    b_q = torch.zeros(num_outputs, device=device)\n",
    "    # Attach gradients\n",
    "    params = [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc,\n",
    "              b_c, W_hq, b_q]\n",
    "    for param in params:\n",
    "        param.requires_grad_(True)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_lstm_state(batch_size, num_hiddens, device):\n",
    "    return (torch.zeros((batch_size, num_hiddens), device=device),\n",
    "            torch.zeros((batch_size, num_hiddens), device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm(inputs, state, params):\n",
    "    [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c,\n",
    "     W_hq, b_q] = params\n",
    "    (H, C) = state\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        I = torch.sigmoid((X @ W_xi) + (H @ W_hi) + b_i)\n",
    "        F = torch.sigmoid((X @ W_xf) + (H @ W_hf) + b_f)\n",
    "        O = torch.sigmoid((X @ W_xo) + (H @ W_ho) + b_o)\n",
    "        C_tilda = torch.tanh((X @ W_xc) + (H @ W_hc) + b_c)\n",
    "        C = F * C + I * C_tilda\n",
    "        H = O * torch.tanh(C)\n",
    "        Y = (H @ W_hq) + b_q\n",
    "        outputs.append(Y)\n",
    "    return torch.cat(outputs, dim=0), (H, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time traveller                                                  \n",
      "time traveller                                                  \n",
      "time traveller  t t t t t t t t t t t t t t t t t t t t t t t t \n",
      "time traveller at at at at at at at at at at at at at at at at a\n",
      "time traveller ate ate ate ate ate ate ate ate ate ate ate ate a\n",
      "time travellere the ate ate ate ate ate ate ate ate ate ate ate \n",
      "time traveller an the the the the the the the the the the the th\n",
      "time travellere the the the the the the the the the the the the \n",
      "time travellere the the the the the the the the the the the the \n",
      "time travellere the the the the the the the the the the the the \n",
      "time travellere the the the the the the the the the the the the \n",
      "time travellere the the the the the the the the the the the the \n",
      "time traveller an the the the the the the the the the the the th\n",
      "time travellerererererererererererererererererererererererererer\n",
      "time traveller and the the the the the the the the the the the t\n",
      "time traveller and the the the the the the the the the the the t\n",
      "time traveller the the the the the the the the the the the the t\n",
      "time travellerere and and the the the the the the the the the th\n",
      "time traveller the the the the the the the the the the the the t\n",
      "time traveller the the the tree tree tree the tree tree tree the\n",
      "time traveller and the thee thee time and and the thee thee time\n",
      "time traveller the the the the the the the the the the the the t\n",
      "time traveller this the trought red the trought red the trought \n",
      "time traveller the time traveller the time traveller the time tr\n",
      "time traveller thin this so in the time traveller thing this so \n",
      "time traveller the there the theed the thee the this the the the\n",
      "time traveller the the the and the the this the this the this th\n",
      "time traveller theeed and and the time traveller theeed and and \n",
      "time traveller theeed and and the this the ground and this is al\n",
      "time traveller hat has and the this the other thing the thonght \n",
      "time traveller three dimensions of the thing the time traveller \n",
      "time traveller the the others had had and and and and and and an\n",
      "time traveller thered dis is it is and the time traveller for th\n",
      "time traveller three dimensions of the indtand have of the three\n",
      "time traveller the time travel ex a forine to said the time trav\n",
      "time traveller the time traveller for the this ge time traveller\n",
      "time traveller three dimensions expot ter it the mome travel ex \n",
      "time traveller threed in trovele that in a midons of the enother\n",
      "time traveller threed the time traveller freed the time travelle\n",
      "time traveller the threes so ot ho le greal to diftersthongo sec\n",
      "time traveller the threg the very siscarifionascer the rers of c\n",
      "time traveller then this now and that is ald have at an wo hade \n",
      "time traveller three dimensions they could mover at up i said th\n",
      "time traveller rechenith sadtanting thing the eoomthr our and th\n",
      "time traveller the thre wron which of the histlars in aud in was\n",
      "time traveller scould the gromenal man go therearme leares the t\n",
      "time traveller but souen this me and andustacincuppes sharight l\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "perplexity 1.1, 8960.0 tokens/sec on cuda:0\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "traveller sat a said the prasod ac matently in our monet ot\n"
     ]
    }
   ],
   "source": [
    "vocab_size, num_hiddens = len(vocab), 256\n",
    "num_epochs, lr = 500, 1\n",
    "model = custom.RNNModelScratch(len(vocab), num_hiddens, device, get_lstm_params,\n",
    "                            init_lstm_state, lstm)\n",
    "custom.train_ch8(model, train_iter, vocab, lr, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    \"\"\"The RNN model.\"\"\"\n",
    "    def __init__(self, rnn_layer, vocab_size, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        self.rnn = rnn_layer\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_hiddens = self.rnn.hidden_size\n",
    "        # If the RNN is bidirectional (to be introduced later),\n",
    "        # `num_directions` should be 2, else it should be 1.\n",
    "        if not self.rnn.bidirectional:\n",
    "            self.num_directions = 1\n",
    "            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)\n",
    "        else:\n",
    "            self.num_directions = 2\n",
    "            self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)\n",
    "\n",
    "    def forward(self, inputs, state):\n",
    "        X = F.one_hot(inputs.T.long(), self.vocab_size)\n",
    "        X = X.to(torch.float32)\n",
    "        Y, state = self.rnn(X, state)\n",
    "        # The fully connected layer will first change the shape of `Y` to\n",
    "        # (`num_steps` * `batch_size`, `num_hiddens`). Its output shape is\n",
    "        # (`num_steps` * `batch_size`, `vocab_size`).\n",
    "        output = self.linear(Y.reshape((-1, Y.shape[-1])))\n",
    "        return output, state\n",
    "\n",
    "    def begin_state(self, device, batch_size=1):\n",
    "        if not isinstance(self.rnn, nn.LSTM):\n",
    "            # `nn.GRU` takes a tensor as hidden state\n",
    "            return  torch.zeros((self.num_directions * self.rnn.num_layers,\n",
    "                                 batch_size, self.num_hiddens),\n",
    "                                device=device)\n",
    "        else:\n",
    "            # `nn.LSTM` takes a tuple of hidden states\n",
    "            return (torch.zeros((\n",
    "                self.num_directions * self.rnn.num_layers,\n",
    "                batch_size, self.num_hiddens), device=device),\n",
    "                    torch.zeros((\n",
    "                        self.num_directions * self.rnn.num_layers,\n",
    "                        batch_size, self.num_hiddens), device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time traveller                                                  \n",
      "time traveller t a t a t a t a t a t a t a t a t a t a t a t a t\n",
      "time travellere the te ate the te ate the te ate the te ate the \n",
      "time travellere an an an an an an an an an an an an an an an an \n",
      "time traveller an the the the the the the the the the the the th\n",
      "time travellerererererererererererererererererererererererererer\n",
      "time traveller the the the the the the the the the the the the t\n",
      "time traveller the the the the the the the the the the the the t\n",
      "time travellere the the the the the the the the the the the the \n",
      "time traveller and the the the the the the the the the the the t\n",
      "time traveller the the the the the the the the the the the the t\n",
      "time traveller and and the and the and the the and and the and t\n",
      "time traveller the the the the the the the the the the the the t\n",
      "time traveller the the the mere the the the the the the the the \n",
      "time traveller the the the and the the the the the the the the t\n",
      "time traveller the aller the aller and the theee the time travel\n",
      "time traveller thing the medine trovel thee it and and there is \n",
      "time traveller thing the fimentions of the thing the fimet in th\n",
      "time travellerical and the this the month of the thing time is i\n",
      "time traveller three dimensions of the thing time aime trovel th\n",
      "time travellery a dintion said the provel to meris and the this \n",
      "time travellery contens that a s mome there is a folly the three\n",
      "time traveller cutter and why three dimensions of space and a fi\n",
      "time traveller whthe gers of space there is three is an all the \n",
      "time traveller space for master of a manisteofinissoon this int \n",
      "time traveller three dimensions and this in ally made to sall pr\n",
      "time traveller for and the time traveller for so it nos erman ab\n",
      "time travellerit s ald dimensions of space weny litherest tirder\n",
      "time traveller for and tho lepe this the germay scattars hard di\n",
      "time travellerit s accont of the grave fured indmattic loon they\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time travellerif legpres shather a cuntl coneraller the instant \n",
      "time traveller free do not rean the thing that ther thing seitit\n",
      "time travelleryou can show black is white back at reve about in \n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time traveller sume was expention in this intarand the not rart \n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time travelleryou can show black is white by argument said filby\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time travelleryou can show black is white by argument said filby\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "time travelleryou can show black is white by argument said filby\n",
      "perplexity 1.0, 8960.0 tokens/sec on cuda:0\n",
      "time travelleryou can show black is white by argument said filby\n",
      "travelleryou can show black is white by argument said filby\n"
     ]
    }
   ],
   "source": [
    "num_inputs = vocab_size\n",
    "lstm_layer = nn.LSTM(num_inputs, num_hiddens)\n",
    "model = RNNModel(lstm_layer, len(vocab))\n",
    "model = model.to(device)\n",
    "custom.train_ch8(model, train_iter, vocab, lr, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
