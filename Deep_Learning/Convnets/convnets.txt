PATHS:
D:\
%HOMEPATH%

----28*28

1) Convolution with 5*5:
=>> n*24*24
2) ReLU
3) Maxpooling 2*2
=? n*12*12
4) Convolution with 5*5:
=>> n*8*8
5) ReLU
6) MaxPooling 2*2
=>> n*4*4
7) FC with n*4*4 : 50
=> 50*1
8) FC with 50*10
9) log_softmax()
if padding is 2

1)n*26*26
2)max_pooling => n*13*13
3)n*9*9
4)max_pooling => n*5*5

if convolution with 3*3:wwith padding =2
1) n*28*28
2) Max_pooling with 2*2 => n*14*14
3) n*12*12 if with 5*5 then n*10*10
4) n*6*6 then n*4*4


reverse first 5 then 3 =>
1) n*26*26
2) n*13*13
3) n*11*11
4) n*6*6

more 3*3
5) n*4*4
6) n*2*2


----Counting Paramters:

with n = 6

Input layer=> 0
Conv Layer => ((5*5*1)+1)*6 = 156
pooling => 0
Conv Layer2 => ((5*5*6)+1)*6 = 906
pooling => 0
FC1 => ((6*4*4)+1)	*50 = 4850
softmax => (10*50)+1*10 = 510

total => 6422!!


#################

Notes::

Some properties:
1) Locality
2) Stationarity
3) Compositionality

===>>

* In detection many methods doesnt be overly concerned with the precise 
  location of the object in the image.
* CNNs systematize this idea of spatial invariance, exploiting it to
  learn useful representations with fewer parameters.
* We need network to use Stationarity(translation invariance),
  Compositionality and locality properties....Which together know as  
  properties of natural properties.
* Let us invoke Locality first:
  Localiy => Sparsity
  *  If our data exhibits locality, each neuron needs to be connected
     to only a few local neurons of the previous layer.
  *  Thus number of connections reduces...
  Stationarity => paramter Sharing
  *   Because of this property we use same a small set of parameters 
      multiple times across the network architecture.
  Advantages of Parameter Sharing and Sparsity:
      * Paramter Sharing :
	 - Faster Convergence
	 - Better Generalisation
	 - not constained to input size
	 - kernel indepence â‡’> high parallelisation
      * Sparsity :
	 - reduced amount of computation
* Convolution:

* Refer .ipynb
* Feature map: the convolutional layer output is sometimes called a feature map.
* Receptive field: receptive field refers to all the elements 
  (from all the previous layers) that may affect the calculation of  x  
  during the forward propagation.



##################
###################
Shared weights
Pooling
Dropouts
###################

MODERN-NETS::
################
LeNet /
AlexNet / 
ZFNet / 
GoogLeNet /
VGGNet /
ResNet /
Nin /
Densenet /

..
Inception Net.
Xception Net.

##################


Alexnet:
let image be of 256*256
1) convnet_1 => 
padding = 2 => 260*260
kernel = 11*11 , stride = 4 ,output_channels = 64 => 64@82*82
Maxpooling with 3*3 and stride = 2 => 64@41*41
2) convnet_2 =>
padding = 2 => 64@45*45
kernel = 5*5, output_channels = 194 => 194@41*41
Maxpooling with 3*3 and stride = 2 => 194@20*20
3)convnet_3 =>
padding = 1  => 194@22*22
kernel = 3*3, output_channels = 384@20*20
4)convnet_4 =>
padding = 1 ,output_channels = 256,kernel = 3*3 => 256@20*20
5)convnet_5 =>
padding = 1 ,output_channels = 256,kernel = 3*3 => 256@20*20
-> Maxpooling-kernel=3,stride=2 => 256@10*10
